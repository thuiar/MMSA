{
  "models": {
    "LF_DNN": {
      "title": "Benchmarking Multimodal Sentiment Analysis",
      "paper_url": "https://link.springer.com/chapter/10.1007/978-3-319-77116-8_13",
      "citation": "",
      "description": "Late Fusion Network."
    },
    "TFN": {
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "paper_url": "https://www.aclweb.org/anthology/D17-1115.pdf",
      "citation": "",
      "description": "Tensor Fusion Network."
    },
    "EF_LSTM": {
      "title": "Recognizing Emotions in Video Using Multimodal DNN Feature Fusion",
      "paper_url": "https://www.aclweb.org/anthology/W18-3302.pdf",
      "citation": "",
      "description": "Early Fusion Network Using LSTM."
    },
    "LMF": {
      "title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors",
      "paper_url": "https://www.aclweb.org/anthology/P18-1209.pdf",
      "citation": "",
      "description": "Low-rank Memory Fusion Network."
    },
    "MFN": {
      "title": "Memory Fusion Network for Multi-View Sequential Learning",
      "paper_url": "https://arxiv.org/abs/1802.00927",
      "citation": "",
      "description": "Memory Fusion Network."
    },
    "Graph_MFN": {
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "paper_url": "https://www.aclweb.org/anthology/P18-1208.pdf",
      "citation": "",
      "description": "Dynamic Fusin Graph after Memory Fusion Network."
    },
    "MFM": {
      "title": "Memory fusion network for multiview sequential learning",
      "paper_url": "https://ojs.aaai.org/index.php/AAAI/article/view/12021",
      "citation": "",
      "description": "Memory fusion network for multiview sequential learning"
    },
    "MulT": {
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "paper_url": "https://github.com/yaohungt/Multimodal-Transformer",
      "citation": "",
      "description": "Multimodal Transformer for Unaligned Multimodal Language Sequences"
    },
    "BERT_MAG": {
      "title": "Integrating multimodal information in large pretrained transformers",
      "paper_url": "https://aclanthology.org/2020.acl-main.214/",
      "citation": "",
      "description": ""
    },
    "MISA": {
      "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis",
      "paper_url": "https://github.com/declare-lab/MISA",
      "citation": "",
      "description": "Modality-Invariant and -Specific Representations"
    },
    "SELF_MM": {
      "title": "Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis",
      "paper_url": "https://ojs.aaai.org/index.php/AAAI/article/view/17289",
      "citation": "",
      "description": ""
    },
    "MTFN": {
      "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality",
      "paper_url": "https://www.aclweb.org/anthology/2020.acl-main.343.pdf",
      "citation": "",
      "description": "Multi-task Multimodal Learning Framework for TFN."
    },
    "MLF_DNN": {
      "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality",
      "paper_url": "https://www.aclweb.org/anthology/2020.acl-main.343.pdf",
      "citation": "",
      "description": "Multi-task Multimodal Learning Framework for LF_DNN."
    },
    "MLMF": {
      "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality",
      "paper_url": "https://www.aclweb.org/anthology/2020.acl-main.343.pdf",
      "citation": "",
      "description": "Multi-task Multimodal Learning Framework for LMF."
    },
    "MMIM": {
      "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
      "paper_url": "https://arxiv.org/pdf/2109.00412.pdf",
      "citation": "",
      "description": "MultiModal-InfoMax"
    },
    "TETFN": {
      "title": "TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis",
      "paper_url": "https://www.sciencedirect.com/science/article/pii/S0031320322007385",
      "citation": "",
      "description": ""
    },
    "CENET": {
      "title": "Cross-modal Enhancement Network for Multimodal Sentiment Analysis",
      "paper_url": "https://ieeexplore.ieee.org/abstract/document/9797846",
      "citation": "",
      "description": ""
    }
  },
  "datasets": {
    "SIMS": {
      "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality",
      "paper_url": "https://www.aclweb.org/anthology/2020.acl-main.343.pdf",
      "citation": "",
      "description": "Chinese Multimodal Sentiment Analysis Dataset."
    },
    "MOSI": {
      "title": "Multimodal Sentiment Intensity Analysis in Videos: Facial Gestures and Verbal Messages",
      "paper_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7742221",
      "citation": "",
      "description": "The CMU-MOSI Datset."
    },
    "MOSEI": {
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "paper_url": "https://aclanthology.org/P18-1208.pdf",
      "citation": "",
      "description": "The CMU-MOSEI Datset."
    },
    "SIMSv2": {
      "title": "Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module",
      "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3536221.3556630",
      "citation": "",
      "description": "CH-SIMSv2: Chinese Multimodal Sentiment Analysis Dataset."
    }
  }
}
